{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyN7Loa0mP+mVjqXJM9M8ZiE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **Educational Historical Chatbot**\n","\n","A specialized chatbot built using the T5 transformer model, fine-tuned on historical Q&A data to provide accurate and informative responses about various historical topics, figures, and events.\n"],"metadata":{"id":"TvFmSoB6A340"}},{"cell_type":"markdown","source":["# 1. Importing Dependencies"],"metadata":{"id":"Y63VwsLVAgIh"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import tensorflow as tf\n","import nltk\n","import langdetect\n","from transformers import T5Tokenizer, TFT5ForConditionalGeneration\n","from transformers import create_optimizer\n","from sklearn.model_selection import train_test_split\n","from tf_keras.src.callbacks import EarlyStopping, ModelCheckpoint\n","from datasets import Dataset"],"metadata":{"id":"wjEPv-zOS8QT","executionInfo":{"status":"ok","timestamp":1740568660400,"user_tz":-120,"elapsed":16876,"user":{"displayName":"Elvis Guy Bakunzi","userId":"02359503838652562430"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Download NLTK resources\n","nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VU7_9C5oVKO5","executionInfo":{"status":"ok","timestamp":1740568663820,"user_tz":-120,"elapsed":516,"user":{"displayName":"Elvis Guy Bakunzi","userId":"02359503838652562430"}},"outputId":"79dea5ae-7827-4534-d98c-a6b4b2732ab3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["# 2. Mount Google Drive and Load Data"],"metadata":{"id":"I1ADcRfWBKER"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iRizesqBTu_8","executionInfo":{"status":"ok","timestamp":1740568690920,"user_tz":-120,"elapsed":24496,"user":{"displayName":"Elvis Guy Bakunzi","userId":"02359503838652562430"}},"outputId":"16b98ccf-11c2-4eca-f5fe-738d9413d040"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["history_data = \"/content/drive/MyDrive/2025/data/history_data_20000.csv\""],"metadata":{"id":"xg_m61jQTv4I","executionInfo":{"status":"ok","timestamp":1740568696937,"user_tz":-120,"elapsed":29,"user":{"displayName":"Elvis Guy Bakunzi","userId":"02359503838652562430"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["# 3. Tokinezation of the data and Fine-tune the Model\n","\n","This includes:\n","\n","- **Data Collection:** Historical Q&A pairs from various sources\n","- **Preprocessing:** Formatting inputs as \"question: {text}\" and targets as direct answers\n","- **Fine-tuning:** Using the T5-small base model with custom - hyperparameters\n","- **Evaluation:** Testing on a held-out dataset with BLEU, ROUGE, and exact match metrics"],"metadata":{"id":"INDbjlSsBc4N"}},{"cell_type":"code","source":["# 1. Load and preprocess the dataset for CSV\n","def load_dataset(file_path):\n","    import pandas as pd\n","\n","    # Load CSV file\n","    df = pd.read_csv(file_path)\n","\n","    # Ensure required columns exist\n","    if 'user' not in df.columns or 'bot' not in df.columns:\n","        raise ValueError(\"CSV file must contain 'user' and 'bot' columns\")\n","\n","    # Format input-output pairs **without `</s>`**\n","    df['input_text'] = df['user'].apply(lambda x: f\"question: {x}\")\n","    df['target_text'] = df['bot']  # No need to append `</s>`\n","\n","    return df\n","\n","def tokenize_data(df, tokenizer, max_input_length=512, max_target_length=128):\n","    input_texts = df['input_text'].tolist()\n","    target_texts = df['target_text'].tolist()\n","\n","    inputs = tokenizer(\n","        input_texts,\n","        truncation=True,\n","        padding='max_length',\n","        max_length=max_input_length,\n","        return_tensors='tf'\n","    )\n","    # Convert target texts to strings before tokenization\n","    target_texts = [str(text) for text in df['target_text'].tolist()]\n","\n","    targets = tokenizer(\n","        target_texts,\n","        truncation=True,\n","        padding='max_length',\n","        max_length=max_target_length,\n","        return_tensors='tf'\n","    )\n","\n","    dataset = tf.data.Dataset.from_tensor_slices((\n","        {\n","            'input_ids': inputs['input_ids'],\n","            'attention_mask': inputs['attention_mask'],\n","            'decoder_input_ids': targets['input_ids'][:, :-1]  # Shifted left for decoder\n","        },\n","        targets['input_ids'][:, 1:]  # Shifted right as labels\n","    ))\n","\n","    # Make sure labels are explicitly added to the dataset\n","    dataset = dataset.map(lambda x, y: ({**x, 'labels': y}, y))\n","\n","    return dataset\n","\n","# 3. Fine-tune the model using TensorFlow methods\n","def fine_tune_model(train_dataset, val_dataset, model_name='t5-small', epochs=6, batch_size=4):\n","    # Load pre-trained model and tokenizer\n","    tokenizer = T5Tokenizer.from_pretrained(model_name)\n","    model = TFT5ForConditionalGeneration.from_pretrained(model_name)\n","\n","    # Prepare batched datasets\n","    train_dataset = train_dataset.shuffle(1000).batch(batch_size)\n","    val_dataset = val_dataset.batch(batch_size)\n","\n","    # Number of training steps\n","    num_train_steps = len(list(train_dataset)) * epochs\n","\n","    optimizer, lr_schedule = create_optimizer(\n","        init_lr=1e-4,\n","        num_train_steps=num_train_steps,\n","        num_warmup_steps=int(0.1 * num_train_steps),\n","        weight_decay_rate=0.01,\n","    )\n","\n","    # Compile the model (ensure 'labels' is specified for loss calculation)\n","    model.compile(optimizer=optimizer)\n","\n","    # Set up callbacks\n","    callbacks = [\n","        EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True),\n","        ModelCheckpoint(\n","            filepath='./checkpoints/model_{epoch}.keras',\n","            save_best_only=True,\n","            monitor='val_loss'\n","        )\n","    ]\n","\n","    # Train the model\n","    history = model.fit(\n","        train_dataset,\n","        validation_data=val_dataset,\n","        epochs=epochs,\n","        callbacks=callbacks\n","    )\n","\n","    # Save the model\n","    model.save_pretrained('/content/drive/MyDrive/2025/model/history_chatbot_model')\n","    tokenizer.save_pretrained('/content/drive/MyDrive/2025/model/history_chatbot_model')\n","\n","    return model, tokenizer, history\n"],"metadata":{"id":"hhoXHuerL8iA","executionInfo":{"status":"ok","timestamp":1740568698541,"user_tz":-120,"elapsed":5,"user":{"displayName":"Elvis Guy Bakunzi","userId":"02359503838652562430"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["# 4. Generate Responses to verify how the model is performing"],"metadata":{"id":"AHG23N__CFug"}},{"cell_type":"code","source":["# 4. Generate responses\n","def generate_response(user_input, model, tokenizer, max_length=100):\n","    input_text = f\"question: {user_input}\"\n","\n","    input_ids = tokenizer(input_text, return_tensors='tf').input_ids\n","\n","    outputs = model.generate(\n","        input_ids,\n","        max_length=max_length,\n","        num_return_sequences=1,\n","        no_repeat_ngram_size=2,  # Prevent repeated phrases\n","        top_k=50,\n","        top_p=0.90,   # Lower top_p for more deterministic responses\n","        temperature=0.6,  # Reduce randomness for accuracy\n","        do_sample=True\n","    )\n","\n","    bot_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    return bot_response\n","\n","# 5. Evaluate the model\n","def evaluate_chatbot(test_data, model, tokenizer):\n","    responses = []\n","    references = []\n","\n","    for _, row in test_data.iterrows():\n","        user_input = row['user']\n","        reference = row['bot']\n","\n","        generated = generate_response(user_input, model, tokenizer)\n","\n","        responses.append(generated)\n","        references.append(reference)\n","\n","    # Calculate BLEU score\n","    try:\n","        import nltk\n","        from nltk.translate.bleu_score import corpus_bleu\n","\n","        # Download necessary NLTK data if not present\n","        try:\n","            nltk.data.find('tokenizers/punkt')\n","        except LookupError:\n","            nltk.download('punkt')\n","\n","        references_tokenized = [[ref.split()] for ref in references]\n","        responses_tokenized = [resp.split() for resp in responses]\n","\n","        bleu_score = corpus_bleu(references_tokenized, responses_tokenized)\n","        print(f\"BLEU Score: {bleu_score}\")\n","    except:\n","        print(\"Couldn't calculate BLEU score. Continuing with other metrics.\")\n","        bleu_score = 0\n","\n","    # Calculate ROUGE score\n","    try:\n","        from rouge_score import rouge_scorer\n","        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n","\n","        rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n","        for ref, gen in zip(references, responses):\n","            score = scorer.score(ref, gen)\n","            for key in rouge_scores:\n","                rouge_scores[key] += score[key].fmeasure\n","\n","        # Average the scores\n","        for key in rouge_scores:\n","            rouge_scores[key] /= len(references)\n","\n","        print(f\"ROUGE Scores: {rouge_scores}\")\n","    except:\n","        print(\"Couldn't calculate ROUGE scores. Continuing with other metrics.\")\n","        rouge_scores = {'rouge1': 0, 'rouge2': 0, 'rougeL': 0}\n","\n","    # Simple exact match accuracy\n","    exact_matches = sum(1 for gen, ref in zip(responses, references) if gen == ref)\n","    accuracy = exact_matches / len(responses)\n","    print(f\"Exact Match Accuracy: {accuracy}\")\n","\n","    return bleu_score, rouge_scores, accuracy, responses\n","\n","# 6. Create a simple web interface using Gradio\n","def create_web_interface(model, tokenizer):\n","    import gradio as gr\n","\n","    def chatbot_interface(user_input):\n","        if not user_input.strip():\n","            return \"Please ask a history question.\"\n","\n","        # Check if question is history-related\n","        history_keywords = [\n","            \"history\", \"ancient\", \"medieval\", \"century\", \"war\", \"empire\", \"king\",\n","            \"queen\", \"civilization\", \"revolution\", \"world war\", \"dynasty\", \"emperor\",\n","            \"archaeological\", \"historical\", \"middle ages\", \"renaissance\", \"prehistoric\",\n","            \"civil war\", \"cold war\", \"rome\", \"egypt\", \"greece\", \"china\", \"mesopotamia\",\n","            \"pharaoh\", \"caesar\", \"viking\", \"ottoman\", \"byzantine\", \"mongol\", \"crusade\",\n","            \"independence\", \"conquest\", \"colonization\", \"monarchy\", \"republic\"\n","        ]\n","\n","        # Very basic domain filtering\n","        is_history_related = any(keyword.lower() in user_input.lower() for keyword in history_keywords)\n","\n","        if not is_history_related:\n","            return \"I'm a history chatbot. Please ask me about historical events, figures, or periods.\"\n","\n","        response = generate_response(user_input, model, tokenizer)\n","        return response\n","\n","    demo = gr.Interface(\n","        fn=chatbot_interface,\n","        inputs=gr.Textbox(lines=2, placeholder=\"Ask me about history...\"),\n","        outputs=\"text\",\n","        title=\"Historical Knowledge Chatbot (T5)\",\n","        description=\"Ask questions about historical events, figures, and periods.\",\n","        examples=[\n","            [\"Who was Cleopatra?\"],\n","            [\"Tell me about the fall of the Roman Empire.\"],\n","            [\"What caused World War I?\"],\n","            [\"Explain the significance of the Renaissance.\"],\n","            [\"Who was Genghis Khan?\"]\n","        ]\n","    )\n","\n","    return demo"],"metadata":{"id":"OHE199TDL-uT","executionInfo":{"status":"ok","timestamp":1740568702491,"user_tz":-120,"elapsed":2,"user":{"displayName":"Elvis Guy Bakunzi","userId":"02359503838652562430"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["df = load_dataset(history_data)\n","print(df[['input_text', 'target_text']].head(10))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UnMx4uEmvnYK","executionInfo":{"status":"ok","timestamp":1740568707615,"user_tz":-120,"elapsed":710,"user":{"displayName":"Elvis Guy Bakunzi","userId":"02359503838652562430"}},"outputId":"96206a3f-cfa1-46a0-9d44-0c882bbb9b9e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["                                          input_text  \\\n","0                                    question: Hello   \n","1                                       question: Hi   \n","2                                question: Hey there   \n","3                                  question: Goodbye   \n","4                            question: See you later   \n","5                       question: Who was Cleopatra?   \n","6  question: Tell me about the fall of the Roman ...   \n","7  question: Tell me about the fall of the Roman ...   \n","8     question: What was life like in ancient Egypt?   \n","9  question: What was the significance of the Mag...   \n","\n","                                         target_text  \n","0  Hi there! Welcome to HistoryBot. I'm here to h...  \n","1  Hello! I'm your guide to the past. Whether you...  \n","2  Hey! Ready to dive into history? From the pyra...  \n","3  Goodbye! Thank you for exploring history with ...  \n","4  See you later! The chronicles of history will ...  \n","5  Cleopatra VII (69-30 BCE) was the last active ...  \n","6  The fall of the Roman Empire was a complex, ce...  \n","7  The fall of the Roman Empire was a complex, ce...  \n","8  Ancient Egypt was a highly structured civiliza...  \n","9  The Magna Carta (Great Charter), signed by Kin...  \n"]}]},{"cell_type":"markdown","source":["# 5. Run all the function to Train the Model"],"metadata":{"id":"9zLhCSuSCblQ"}},{"cell_type":"code","source":["# Main execution flow\n","if __name__ == \"__main__\":\n","\n","    # Load and preprocess data from CSV\n","    df = load_dataset(history_data)\n","\n","    # Split the data\n","    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","    train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=42)\n","\n","    # Initialize tokenizer\n","    tokenizer = T5Tokenizer.from_pretrained('t5-small')\n","\n","    # Tokenize data\n","    train_dataset = tokenize_data(train_df, tokenizer)\n","    val_dataset = tokenize_data(val_df, tokenizer)\n","\n","    # Fine-tune the model\n","    model, tokenizer, history = fine_tune_model(\n","        train_dataset,\n","        val_dataset,\n","        model_name='t5-small',  # You can use 't5-base' for better results\n","        epochs=6,\n","        batch_size=4\n","    )\n","\n","    # Evaluate the model\n","    bleu_score, rouge_scores, accuracy, responses = evaluate_chatbot(test_df, model, tokenizer)\n","\n","    # Save evaluation results\n","    evaluation_results = {\n","        'bleu_score': bleu_score,\n","        'rouge_scores': rouge_scores,\n","        'accuracy': accuracy,\n","        'examples': [\n","            {\n","                'user_input': row['user'],\n","                'reference': row['bot'],\n","                'generated': resp\n","            }\n","            for (_, row), resp in zip(test_df.iterrows(), responses)\n","        ]\n","    }\n","\n","    import json\n","    with open('/content/drive/MyDrive/2025/data/evaluation_results.json', 'w') as f:\n","        json.dump(evaluation_results, f, indent=2)\n","\n","    # Create web interface\n","    demo = create_web_interface(model, tokenizer)\n","    demo.launch()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"3Q-q2CirMGOR","executionInfo":{"status":"ok","timestamp":1740539768256,"user_tz":-120,"elapsed":9371649,"user":{"displayName":"Elvis Guy Bakunzi","userId":"02359503838652562430"}},"outputId":"f4833478-b5bc-44fb-87f0-b157f8d9fbf9"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n","\n","All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/6\n","1119/1119 [==============================] - ETA: 0s - loss: 1.8250"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/generation/tf_utils.py:465: UserWarning: `seed_generator` is deprecated and will be removed in a future version.\n","  warnings.warn(\"`seed_generator` is deprecated and will be removed in a future version.\", UserWarning)\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r1119/1119 [==============================] - 269s 220ms/step - loss: 1.8250 - val_loss: 0.4977\n","Epoch 2/6\n","1119/1119 [==============================] - 242s 217ms/step - loss: 0.4883 - val_loss: 0.4688\n","Epoch 3/6\n","1119/1119 [==============================] - 242s 216ms/step - loss: 0.4577 - val_loss: 0.4565\n","Epoch 4/6\n","1119/1119 [==============================] - 243s 218ms/step - loss: 0.4410 - val_loss: 0.4493\n","Epoch 5/6\n","1119/1119 [==============================] - 242s 217ms/step - loss: 0.4292 - val_loss: 0.4460\n","Epoch 6/6\n","1119/1119 [==============================] - 243s 217ms/step - loss: 0.4224 - val_loss: 0.4446\n","BLEU Score: 0.039384050217590734\n","ROUGE Scores: {'rouge1': 0.09020043868587588, 'rouge2': 0.03760325604688853, 'rougeL': 0.07707438696990156}\n","Exact Match Accuracy: 0.0\n","Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://c1a690206e92668d20.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://c1a690206e92668d20.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}}]}]}